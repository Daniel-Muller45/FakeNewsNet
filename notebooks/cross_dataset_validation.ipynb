{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Dataset Validation for Fake News Detection\n",
    "\n",
    "This notebook implements cross-dataset validation by training a fake news detection model on the BuzzFeed dataset and testing it on the PolitiFact dataset.\n",
    "\n",
    "This approach helps evaluate how well the model generalizes to new sources, which is crucial for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "We'll load two datasets:\n",
    "- BuzzFeed dataset for training\n",
    "- PolitiFact dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BuzzFeed datasets for training\n",
    "bf_fake_df = pd.read_csv(\"../data/BuzzFeed_fake_news_content.csv\")\n",
    "bf_real_df = pd.read_csv(\"../data/BuzzFeed_real_news_content.csv\")\n",
    "\n",
    "# Load PolitiFact datasets for testing\n",
    "pf_fake_df = pd.read_csv(\"../data/PolitiFact_fake_news_content.csv\")\n",
    "pf_real_df = pd.read_csv(\"../data/PolitiFact_real_news_content.csv\")\n",
    "\n",
    "# Print the sizes of each dataset\n",
    "print(f\"BuzzFeed fake news: {len(bf_fake_df)} articles\")\n",
    "print(f\"BuzzFeed real news: {len(bf_real_df)} articles\")\n",
    "print(f\"PolitiFact fake news: {len(pf_fake_df)} articles\")\n",
    "print(f\"PolitiFact real news: {len(pf_real_df)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Labels and Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the datasets\n",
    "bf_fake_df['label'] = 1  # 1 for fake news\n",
    "bf_real_df['label'] = 0  # 0 for real news\n",
    "pf_fake_df['label'] = 1\n",
    "pf_real_df['label'] = 0\n",
    "\n",
    "# Combine BuzzFeed datasets for training\n",
    "bf_combined_df = pd.concat([bf_fake_df, bf_real_df], ignore_index=True)\n",
    "# Shuffle the data\n",
    "bf_combined_df = bf_combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Combine PolitiFact datasets for testing\n",
    "pf_combined_df = pd.concat([pf_fake_df, pf_real_df], ignore_index=True)\n",
    "# Shuffle the data\n",
    "pf_combined_df = pf_combined_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Create a combined text feature that includes both the title and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined text field from title and content\n",
    "bf_combined_df['combined_text'] = bf_combined_df['title'].fillna('') + ' ' + bf_combined_df['text'].fillna('')\n",
    "pf_combined_df['combined_text'] = pf_combined_df['title'].fillna('') + ' ' + pf_combined_df['text'].fillna('')\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train = bf_combined_df['combined_text']\n",
    "y_train = bf_combined_df['label']\n",
    "X_test = pf_combined_df['combined_text']\n",
    "y_test = pf_combined_df['label']\n",
    "\n",
    "print(f\"Training data size: {len(X_train)} samples\")\n",
    "print(f\"Testing data size: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Vectorization\n",
    "\n",
    "We'll convert the text data into numerical vectors using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Number of features: {X_train_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000, C=1.0)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Real', 'Fake'], \n",
    "            yticklabels=['Real', 'Fake'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Let's examine which words are most indicative of fake vs. real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "importance = model.coef_[0]\n",
    "\n",
    "# Get top 10 features for real and fake news\n",
    "sorted_idx = importance.argsort()\n",
    "top_fake_features = [(feature_names[idx], importance[idx]) for idx in sorted_idx[-10:]]\n",
    "top_real_features = [(feature_names[idx], importance[idx]) for idx in sorted_idx[:10]]\n",
    "\n",
    "print(\"Top 10 features for fake news:\")\n",
    "for feature, weight in reversed(top_fake_features):\n",
    "    print(f\"{feature}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 features for real news:\")\n",
    "for feature, weight in top_real_features:\n",
    "    print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Top features for fake news\n",
    "plt.subplot(1, 2, 1)\n",
    "y_pos = np.arange(len(top_fake_features))\n",
    "weights = [weight for _, weight in reversed(top_fake_features)]\n",
    "features = [feature for feature, _ in reversed(top_fake_features)]\n",
    "plt.barh(y_pos, weights, align='center')\n",
    "plt.yticks(y_pos, features)\n",
    "plt.xlabel('Weight')\n",
    "plt.title('Top Features for Fake News')\n",
    "\n",
    "# Top features for real news\n",
    "plt.subplot(1, 2, 2)\n",
    "y_pos = np.arange(len(top_real_features))\n",
    "weights = [abs(weight) for _, weight in top_real_features]  # Use absolute values for better visualization\n",
    "features = [feature for feature, _ in top_real_features]\n",
    "plt.barh(y_pos, weights, align='center')\n",
    "plt.yticks(y_pos, features)\n",
    "plt.xlabel('Weight (Absolute Value)')\n",
    "plt.title('Top Features for Real News')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sentiment Analysis (Optional)\n",
    "\n",
    "This section requires the transformers library. If you don't have it installed, run:\n",
    "```\n",
    "pip install transformers torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import transformers for sentiment analysis\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    from scipy.special import softmax\n",
    "    import torch\n",
    "    \n",
    "    # Load the sentiment model\n",
    "    MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    print(\"Transformers library loaded successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Transformers library not found. Skipping sentiment analysis.\")\n",
    "    print(\"To install: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def get_sentiment_scores(text):\n",
    "        \"\"\"Get sentiment scores for a text (returns [negative, neutral, positive])\"\"\"\n",
    "        try:\n",
    "            encoded_text = tokenizer(text[:512], return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                output = sentiment_model(**encoded_text)\n",
    "            scores = output[0][0].numpy()\n",
    "            scores = softmax(scores)\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {e}\")\n",
    "            return np.array([0.33, 0.33, 0.33])  # Default to balanced scores if error\n",
    "    \n",
    "    # Get sentiment scores for a sample of articles (for performance reasons)\n",
    "    sample_size = min(50, len(bf_combined_df))\n",
    "    \n",
    "    # Sample from BuzzFeed training data\n",
    "    print(f\"Analyzing sentiment for {sample_size} sample articles...\")\n",
    "    bf_sample = bf_combined_df.sample(sample_size, random_state=42)\n",
    "    bf_sample['sentiment'] = bf_sample['title'].apply(get_sentiment_scores)\n",
    "    \n",
    "    # Extract sentiment components\n",
    "    bf_sample['negative_score'] = bf_sample['sentiment'].apply(lambda x: x[0])\n",
    "    bf_sample['neutral_score'] = bf_sample['sentiment'].apply(lambda x: x[1])\n",
    "    bf_sample['positive_score'] = bf_sample['sentiment'].apply(lambda x: x[2])\n",
    "    \n",
    "    print(\"Sentiment analysis completed!\")\n",
    "except NameError:\n",
    "    print(\"Skipping sentiment analysis (transformers not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Compare sentiment distributions between real and fake news\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x='label', y='negative_score', data=bf_sample)\n",
    "    plt.title('Negative Sentiment Distribution')\n",
    "    plt.xlabel('Label (0=Real, 1=Fake)')\n",
    "    plt.ylabel('Negative Sentiment Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x='label', y='positive_score', data=bf_sample)\n",
    "    plt.title('Positive Sentiment Distribution')\n",
    "    plt.xlabel('Label (0=Real, 1=Fake)')\n",
    "    plt.ylabel('Positive Sentiment Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print sentiment statistics\n",
    "    print(\"\\nSentiment Analysis on Training Data Sample:\")\n",
    "    print(bf_sample.groupby('label')[['negative_score', 'neutral_score', 'positive_score']].mean())\n",
    "except NameError:\n",
    "    print(\"Skipping sentiment visualization (sentiment analysis not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions\n",
    "\n",
    "This cross-dataset validation experiment demonstrates how well our fake news detection model generalizes to new, unseen sources.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
