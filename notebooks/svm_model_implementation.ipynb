{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine for Fake News Detection\n",
    "\n",
    "This notebook implements a Support Vector Machine (SVM) classifier for fake news detection and compares its performance with other models. SVMs are effective for text classification tasks because they can handle high-dimensional feature spaces well and can capture complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BuzzFeed datasets for training\n",
    "bf_fake_df = pd.read_csv(\"../data/BuzzFeed_fake_news_content.csv\")\n",
    "bf_real_df = pd.read_csv(\"../data/BuzzFeed_real_news_content.csv\")\n",
    "\n",
    "# Load PolitiFact datasets for testing\n",
    "pf_fake_df = pd.read_csv(\"../data/PolitiFact_fake_news_content.csv\")\n",
    "pf_real_df = pd.read_csv(\"../data/PolitiFact_real_news_content.csv\")\n",
    "\n",
    "# Print the sizes of each dataset\n",
    "print(f\"BuzzFeed fake news: {len(bf_fake_df)} articles\")\n",
    "print(f\"BuzzFeed real news: {len(bf_real_df)} articles\")\n",
    "print(f\"PolitiFact fake news: {len(pf_fake_df)} articles\")\n",
    "print(f\"PolitiFact real news: {len(pf_real_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the datasets\n",
    "bf_fake_df['label'] = 1  # 1 for fake news\n",
    "bf_real_df['label'] = 0  # 0 for real news\n",
    "pf_fake_df['label'] = 1\n",
    "pf_real_df['label'] = 0\n",
    "\n",
    "# Combine BuzzFeed datasets for training\n",
    "bf_combined_df = pd.concat([bf_fake_df, bf_real_df], ignore_index=True)\n",
    "# Shuffle the data\n",
    "bf_combined_df = bf_combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Combine PolitiFact datasets for testing\n",
    "pf_combined_df = pd.concat([pf_fake_df, pf_real_df], ignore_index=True)\n",
    "# Shuffle the data\n",
    "pf_combined_df = pf_combined_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined text field from title and content\n",
    "bf_combined_df['combined_text'] = bf_combined_df['title'].fillna('') + ' ' + bf_combined_df['text'].fillna('')\n",
    "pf_combined_df['combined_text'] = pf_combined_df['title'].fillna('') + ' ' + pf_combined_df['text'].fillna('')\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train = bf_combined_df['combined_text']\n",
    "y_train = bf_combined_df['label']\n",
    "X_test = pf_combined_df['combined_text']\n",
    "y_test = pf_combined_df['label']\n",
    "\n",
    "print(f\"Training data size: {len(X_train)} samples\")\n",
    "print(f\"Testing data size: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "# We'll use a smaller number of features to speed up SVM training\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Number of features: {X_train_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Compare Models\n",
    "\n",
    "We'll train three different models and compare their performance:\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest (ensemble method)\n",
    "3. Support Vector Machine (new model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Predicting with {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    predict_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    print(f\"Prediction time: {predict_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Create confusion matrix plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Real', 'Fake'],\n",
    "                yticklabels=['Real', 'Fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_time': train_time,\n",
    "        'predict_time': predict_time,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = [\n",
    "    (LogisticRegression(max_iter=1000, C=1.0), \"Logistic Regression\"),\n",
    "    (RandomForestClassifier(n_estimators=100, random_state=42), \"Random Forest\"),\n",
    "    (SVC(kernel='linear', C=1.0, probability=True), \"Support Vector Machine\")\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "\n",
    "for model, model_name in models:\n",
    "    result = train_evaluate_model(model, model_name, X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "# Accuracy comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='model_name', y='accuracy', data=results_df)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.5, 1.0)  # Start from 0.5 for better visualization\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "# F1 Score comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='model_name', y='f1_score', data=results_df)\n",
    "plt.title('Model F1 Score Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim(0.5, 1.0)  # Start from 0.5 for better visualization\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and prediction time comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='model_name', y='train_time', data=results_df)\n",
    "plt.title('Training Time Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='model_name', y='predict_time', data=results_df)\n",
    "plt.title('Prediction Time Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Prediction Time (seconds)')\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimize SVM with Grid Search (Optional)\n",
    "\n",
    "If the SVM shows promise, we can optimize its hyperparameters for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for SVM\n",
    "# Note: This can be computationally intensive, so we use a small grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "print(\"Starting Grid Search for SVM optimization...\")\n",
    "print(\"Note: This may take some time to complete.\")\n",
    "\n",
    "# You can comment out this cell if you want to skip the grid search\n",
    "grid_search = GridSearchCV(SVC(probability=True), param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the optimized SVM model\n",
    "try:\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    optimized_result = train_evaluate_model(best_svm, \"Optimized SVM\", X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "    \n",
    "    # Add to results\n",
    "    results.append(optimized_result)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df\n",
    "except NameError:\n",
    "    print(\"Grid search was skipped, so no optimized model is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis for SVM\n",
    "\n",
    "For linear SVMs, we can examine the coefficients to understand which features are most important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM for feature analysis\n",
    "linear_svm = SVC(kernel='linear', C=1.0)\n",
    "linear_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = linear_svm.coef_.toarray()[0]\n",
    "\n",
    "# Create DataFrame for feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': coefficients\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by absolute importance\n",
    "feature_importance['abs_importance'] = np.abs(feature_importance['importance'])\n",
    "feature_importance = feature_importance.sort_values('abs_importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top features indicating fake news (positive coefficients)\n",
    "print(\"Top features indicating FAKE news:\")\n",
    "fake_features = feature_importance[feature_importance['importance'] > 0].head(15)\n",
    "for i, row in fake_features.iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nTop features indicating REAL news:\")\n",
    "real_features = feature_importance[feature_importance['importance'] < 0].head(15)\n",
    "for i, row in real_features.iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Top features for fake news\n",
    "plt.subplot(1, 2, 1)\n",
    "fake_features = feature_importance[feature_importance['importance'] > 0].head(10)\n",
    "sns.barplot(x='importance', y='feature', data=fake_features)\n",
    "plt.title('Top Features Indicating Fake News')\n",
    "plt.xlabel('Coefficient Value')\n",
    "\n",
    "# Top features for real news\n",
    "plt.subplot(1, 2, 2)\n",
    "real_features = feature_importance[feature_importance['importance'] < 0].head(10)\n",
    "real_features['importance'] = np.abs(real_features['importance'])  # For better visualization\n",
    "sns.barplot(x='importance', y='feature', data=real_features)\n",
    "plt.title('Top Features Indicating Real News')\n",
    "plt.xlabel('Coefficient Value (Absolute)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "In this notebook, we've implemented and evaluated a Support Vector Machine classifier for fake news detection and compared its performance with Logistic Regression and Random Forest models.\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Performance Comparison**: We can compare how SVM performs against other models in terms of accuracy, F1 score, and computational efficiency.\n",
    "\n",
    "2. **Feature Importance**: The SVM model helps identify which words are most indicative of fake vs. real news, providing insights into the linguistic patterns that characterize each category.\n",
    "\n",
    "3. **Model Selection Tradeoffs**: Different models offer different tradeoffs between accuracy, training time, and prediction speed, which is important for real-world applications.\n",
    "\n",
    "4. **Cross-Dataset Validation**: By training on BuzzFeed and testing on PolitiFact, we've assessed how well the SVM generalizes to new sources compared to other models.\n",
    "\n",
    "The SVM model offers a different approach to fake news detection that may capture patterns not identified by the other models. Depending on the results, you might consider using it as part of an ensemble approach or in situations where its specific strengths are valuable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
